---
title: "HW3-Audit-Classification"
author: "Liping Li (lil112@pitt.edu)"
date: "2017.02.14"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE)
```

# Preparation

### Load function script

```{r functions loading}
source("hw3-functions.R")
# need to write the full path if it is't under working directiory.

# Modified the shared script  (http://www.yurulin.com/class/spring2017_datamining/src/hw3-sample.html)

# Major changes are 
# 1. Add prepare.data module
# 2. Add perfMeasures function to return a list of performance measures
# 3. Add matMeasures function to return cutoff specified measures
# 4. Add parameters(nn for knn neighbor numbers, gamma &cost for svm, cp for tree)
```

### Load relevant R packages

```{r library loading}
# install.packages("ROCR","e1071","rpart","class","randomForest","ada","knitr","data.table","plyr")
library(ROCR) # performance measures and plot
library(e1071) # svm naiveBayes
library(rpart) # decision tree
library(class) # knn
library(ada) # adaBoost
library(plyr) # coding data
library(data.table) # data manipulation
suppressMessages(library(randomForest))# bagging variation
library(knitr) # nice table
```

### Load data

```{r import data}
dataset=load.data("http://www.yurulin.com/class/spring2017_datamining/data/audit.csv")
```

### Prepare data
```{r prepare data}
dataset1=prepare.data(dataset, dummies=T, maskDummies=F,scale=T, y=10)

# Since we need to split data, to avoid incompatible categories levels, I make them dummies first.
# Some methods cannot deal with complicated variable names, so I write maskDummies part in case.
# knn and svm will be impacted by variables value range
# y represent response variable position
```

```{r dataset2}
dataset2=prepare.data(dataset, dummies=T, maskDummies=F,scale=F, y=10)

# Since we need to split data, to avoid incompatible categories levels, I make them dummies first.
# Some methods cannot deal with complicated variable names, so I write maskDummies part in case.
# knn and svm will be impacted by variables value range
# y represent response variable position
```

# 1. All-variables Starter Models

### Logistic regression

Try logistic regression with numeric variable scaled and unscaled data.

```{r all lr}
set.seed(12312)
logistic1 = my.classifier(dataset=dataset1, cl.name = 'lr', do.cv = T)
```

```{r all lr without scale}
set.seed(12312)
logistic2 = my.classifier(dataset=dataset2, cl.name = 'lr', do.cv = T)
```

Results of two dataset are similar. I would use scaled data in the following anaylsis.

When the confusion matrix measures has higher precision, but a very low recall, suggesting **this cutoff is too strict** to label cases as positive(1 in this study).
Since performance function give an average measures, it shows a higher recall and lower precision, demonstrating false positive rate is very low and false negative rate is very high at the cutoff level 0.5. Thus we will probably have a ROC curve that has a very big slope as the left side.

```{r lr ROC, fig.align='center', fig.height=5, fig.width=5}
logisticROC = logistic1$ROC
plot(logisticROC)
```

### KNN

Since it's a big dataset, I start with nn=10.
```{r all knn}
set.seed(12312)
KNN10 = my.classifier(dataset=dataset1, cl.name = 'knn', nn=10, do.cv =T)
```

### Naive Bayes
```{r all nb}
set.seed(12312)
NB = my.classifier(dataset=dataset1, cl.name = 'nb', do.cv = T)
```


### Decision Tree
```{r all dtree}
set.seed(12312)
DecisionTree = my.classifier(dataset=dataset1, cl.name = 'tree', cp=0.001, do.cv = T)
```

### SVM
```{r all svm}
set.seed(12312)
SVM = my.classifier(dataset=dataset1, cl.name = 'svm', kopt='linear',do.cv = T)
```

### Boost Trees (adaBoost)
```{r all adaboost}
set.seed(12312)
AdaBoost = my.classifier(dataset=dataset1, cl.name = 'adaBoost', do.cv = T)
```

### randomForest

I add randomForest to represent one more ensemble learning method. There seem to be some controversies whether randomForest is bagging.It developed based on "bagging" idea and had introduced "feature bagging".

```{r all randomForest}
set.seed(12312)
rf = my.classifier(dataset=dataset1, cl.name = 'randomForest', do.cv = T)
```

### Result Table 1 (cutoff=0.5)
```{r all result table mat}
matResult=rbind(logistic1$matMeasure,KNN10$matMeasure,NB$matMeasure,
                DecisionTree$matMeasure,SVM$matMeasure,AdaBoost$matMeasure,rf$matMeasure)
row.names(matResult) <- c("Logistic","kNN-10","NB","Decision Tree","SVM","adaBoost","Random Forest")
matResult1 = as.data.frame(t(matResult))
kable(matResult1, format = "markdown",digits=2,align="l")
```

### Result Table 2 (performance)
```{r all result table perf}
perfResult=rbind(logistic1$perfMeasure,KNN10$perfMeasure,NB$perfMeasure,
                DecisionTree$perfMeasure,SVM$perfMeasure,AdaBoost$perfMeasure,rf$perfMeasure)
row.names(perfResult) <- c("Logistic","kNN-10","NB","Decision Tree","SVM","adaBoost","Random Forest")
perfResult1 = as.data.frame(t(perfResult))
kable(perfResult1,format = "markdown",digits=2,align="l")
```

In terms of cutoff=0.5, all models have extremely low accuracy except **naive Bayes**. But when it comes to performance averaged accuracy, all models get higher accuracy except naive bayes. For example, randome forest get an accuracy of 0.77. 

This phenomenon is very interesting. I think it maybe because other methods are more or less based on regression calculation and responses in this study are not evenly distributed in 0 and 1. Most cases are 0, making all predictions become close to 0. Thus, cutoff=0.5 is too strict and there will be **high false negative rate**.

But naive Bayes is calculated based probablities, it works fine with cutoff=0.5.


### Result Barchat (performance)
```{r all result fscore bar, fig.align="left",fig.height=4,fig.width=10}
barplot(perfResult$fscore, names.arg = rownames(perfResult),main="10-fold CV F-Score for Classifiers", col="red", border="red")
```

```{r all result auc bar, fig.align="left",fig.height=4,fig.width=10}
barplot(perfResult$auc, names.arg = rownames(perfResult),main="10-fold CV AUC for Classifiers", col="red", border="red")
```

Observing these two barcharts. It seems **Logistic regression** and **adaBoost** have the best averaged performances. Sometimes, simple methods can outperform complicated methods.

And complying with common sense, boost trees(adaBoost) and random forest will be better than the simple Decision Tree.


# 2. Technique Variants

### KNN variants

I set 10 as starter neighbor numbers. How about nn= 3, 5, 20, 30?

```{r knn variants}
set.seed(12312)
KNN3 = my.classifier(dataset=dataset1, cl.name = 'knn', nn=3, do.cv = T)
KNN5 = my.classifier(dataset=dataset1, cl.name = 'knn', nn=5, do.cv = T)
KNN15 = my.classifier(dataset=dataset1, cl.name = 'knn', nn=15, do.cv = T)
KNN30 = my.classifier(dataset=dataset1, cl.name = 'knn', nn=30, do.cv = T)
```

Refering to performance-function-get measures, nn=5 has better f-score, nn=15 has better accuracy and nn=30 has better auc, while nn=3 is poor in all measures. Small segments are vulnerable to overfitting.

### Prune trees

prune function in rpart package has already embedded cross validation. So I will use prune function to get best cp value that minimalize xerror. Then use the cp value to do tree cross validation modeling.

```{r before prune,fig.align="left",fig.height=5,fig.width=10}
set.seed(12312)
model1 = rpart(y~., data=dataset1, method="class",cp=0.001,maxdepth=10 )
par(mfrow=c(1,2)) 
plotcp(model1) # visualize cp result
 ## plot the tree
plot(model1, uniform=TRUE, main="Classification Tree")
text(model1, use.n=TRUE, all=TRUE, cex=.8)
```

```{r prune, fig.align="left"}
set.seed(12312)
model2 = prune(model1, cp=model1$cptable[which.min(model1$cptable[,"xerror"]),"CP"],
                 xval=10)
    # xval for number of cross-validation
    printcp(model2) 
    ## plot the tree
    plot(model2, uniform=TRUE, main="Pruned Classification Tree")
    text(model2, use.n=TRUE, all=TRUE, cex=.8)

```

Here cp is 0.018.

```{r pruned tree}
set.seed(12312)
pruned = my.classifier(dataset = dataset1,cl.name="tree",cp=0.018,do.cv=T)
```

It's actually get poorer performance.

### SVM Variants ###

tune.svm is a tuning function that incorperates 10-fold cross validation. I will use it to find optimal gamma and cost for radial and polynomial kernels.
```{r tune radial svm}
set.seed(12312)
tuned <- tune.svm(y~., data = dataset1, kernel="radial",
                    gamma = 10^(-6:-1), cost = 10^(-1:1))
summary(tuned)
```


```{r radial svm}
set.seed(12312)
  g = tuned[['best.parameters']]$gamma
  c = tuned[['best.parameters']]$cost
radialSVM = my.classifier(dataset = dataset1,cl.name = "svm",kopt="radial",gamma = g,
                          cost = c, do.cv=T)
```

```{r tune polynomial svm}
set.seed(12312)
tuned <- tune.svm(y~., data = dataset1, kernel="polynomial",
                    gamma = 10^(-6:-1), cost = 10^(-1:1))
summary(tuned)
```

```{r polynomial svm}
set.seed(12312)
  g = tuned[['best.parameters']]$gamma
  c = tuned[['best.parameters']]$cost
polySVM = my.classifier(dataset = dataset1,cl.name = "svm",kopt="polynomial",gamma = g,
                          cost = c, do.cv=T)
```

### Add Variants to Previous Result Table 1(cutoff=0.5)
```{r add result table mat}
matResult=rbind(logistic1$matMeasure,NB$matMeasure,KNN3$matMeasure, KNN5$matMeasure,
                KNN10$matMeasure, KNN15$matMeasure,KNN30$matMeasure,
                SVM$matMeasure,radialSVM$matMeasure,polySVM$matMeasure, 
                DecisionTree$matMeasure,pruned$matMeasure,
                AdaBoost$matMeasure,rf$matMeasure) # put trees together

row.names(matResult) <- c("Logistic","NB","KNN3","KNN5","KNN10","KNN15","KNN30",
                        "SVM","radialSVM","polySVM","Tree","PrunedTree",
                        "adaBoost","randomForest")
matResult2 = as.data.frame(t(matResult))
kable(matResult2,format = "markdown",digits=2,align="l")
```

Set cutoff=0.5, naive Bayes still outperform all.

### Add Variants to Previous Result Table 2(performance)
```{r add result table perf}
perfResult=rbind(logistic1$perfMeasure,NB$perfMeasure,KNN3$perfMeasure, KNN5$perfMeasure,
                KNN10$perfMeasure, KNN15$perfMeasure,KNN30$perfMeasure,
                SVM$perfMeasure,radialSVM$perfMeasure,polySVM$perfMeasure, 
                DecisionTree$perfMeasure,pruned$perfMeasure,
                AdaBoost$perfMeasure,rf$perfMeasure) 

row.names(perfResult) <- c("Logistic","NB","KNN3","KNN5","KNN10","KNN15","KNN30",
                        "SVM","radialSVM","polySVM","Tree","PrunedTree",
                        "adaBoost","randomForest")
perfResult2 = as.data.frame(t(perfResult))
kable(perfResult2,format = "markdown",digits=2,align="l")
```

### Result Barchat (performance)
```{r add result fscore bar, fig.align="left",fig.height=4,fig.width=10}
barplot(perfResult$fscore, names.arg = rownames(perfResult),main="10-fold CV F-Score for Classifiers", col="red", border="red")
```

```{r add result auc bar, fig.align="left",fig.height=4,fig.width=10}
barplot(perfResult$auc, names.arg = rownames(perfResult),main="10-fold CV AUC for Classifiers", col="red", border="red")
```

When nn=5, knn method get better result in f-score. But AUC index suggest bigger nn(nn=30) will be better.

My svm default kernel is linear and it has better performance than radial and polynoimal kernal.

Pruned tree has poorer f-score than the original one, because it acutally reduced model complexity.

Boost trees(adaBoost) and Bagged trees(randomForest) give better results than simple decision. tree.


# 3. Compare ROC Curves

```{r compare roc curves, fig.align="left",fig.height=6,fig.width=6}
myPalette <- c("#1B9E77", "#D95F02", "#7570B3" ,"#E7298A", 
               "#E6AB02", "#A6761D", "#666666")
plot(logistic1$ROC, type="l", lwd=3,col=myPalette[1])
plot(NB$ROC, add=T,type="l", lwd=3,col=myPalette[2])
plot(KNN30$ROC, add=T, type="l", lwd=3,col=myPalette[3])
plot(SVM$ROC, add=T, type="l", lwd=3,col=myPalette[4])
plot(DecisionTree$ROC, add=T,type="l", lwd=3, col=myPalette[5])
plot(AdaBoost$ROC, add=T,type="l", lwd=3, col=myPalette[6])
plot(rf$ROC, add=T,type="l", lwd=3, col=myPalette[7])
names = c("Logistic","naiveBayes","KNN30", "SVM","Decision Tree", "adaBoost","randomForest")
legend("bottomright", legend = names, lty = 1, lwd = 3, col = myPalette)
```

Logistic regression, adaBoost and randomForest have smooth ROC curves, while others seem unstable.


# 4. Summary

### naiveBayes

When I set cutoff=0.5, all models get very poor confusion matrix generated measures except naiveBayes. I think it's because naiveBayes is calculated based probablities, a fundamentally distinct way from other methods.


### Logistic Regression

Simple regression is powerful. Though I scaled numeric predictors to 0-1 for the ease of conducting knn and svm, it's robust to the change and yeilds results as good as sophiscated adaBoost and randomForest.


### Ensembles

Both of the two ensembling trees(boosting: adaBoost, bagging variation: randomForest) have averagely better performance than other methods. 

What I missed here is stacking. That needs some time to rewrite the function script to extract probs(predictions) generated by each model. By averaging (or weight voting), I will have a stacked result.


### Average performance measures and cutoff result

Other methods have better measures than naiveBayes in terms of ROCR::performance generated measures. In this study, most samples responds 0(negative). So when cutoff is 0.5, they suffer from high false negative rate problem(low recall). In ROC plots, we can see they have extremely high true positive rate vs low false positive rate at the left side. We can probably get better accuracy index by lower the cutoff.

Let me try it.

```{r 0.3 lr}
set.seed(12312)
logistic1 = my.classifier(dataset=dataset1, cl.name = 'lr',cutoff = 0.001,kfold=10, do.cv = T)
```

Compare with this:
> [1] "Confusion matrix calculated measures:"
accuracy= 0.0785124 precision= 1 recall= 0.0785124 f-score= 0.1455939 
 cutoff= 0.5 
 
About 75% responses are 0(negative). Only when I lower the cutoff will the accuracy be bigger than 0.75.
 